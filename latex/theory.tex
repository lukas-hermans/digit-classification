With the definitions above in mind, the problem in the present paper consists in finding a multiclass predictor $f_S: U \rightarrow \{0, 1, \dots, 9\}, \vec{x} \mapsto \hat{y}$. The subscript $S$ of $f_S$ indicates that the classifier is trained on the training set $S$. In principal, $U$ contains all possible image vectors $\vec{x}$ of the form described above. In other words, for all possible images $\vec{x}$ the classifier outputs a label $\hat{y}$. However, first, the images should be digits from $0$ to $9$ (though they can be blurry, for instance) as the classifier is not trained to identify other objects, such as letters. Second, here, the predicted labels $\hat{y}$ for the images in the test set $D$ are of particular interest because they are used to evaluate the performance of a trained classifier on new images by comparing them to the true labels $y$. The images in the test set $D$ are \enquote{new} in the sense, that they are not included in the training set $S$ that is used to train the classifier $f_S$. \\

From the multiclass classification problem stated above, ten binary classification problems can be derived using \textit{one-vs-all encoding}. For the application of one-vs-all encoding, the initial label $y \in \{0, 1, \dots, 9\}$ is transformed into a binary label $z \in \{-1, 1\}$ by fixing a digit $a \in \{0, 1, \dots, 9\}$. By setting $z = 1$ if $y = a$, and $z = -1$ else, the multiclass training set $S$ is transformed into a binary training set $S^{(a)}$, where $a$ refers to the fixed digit in the one-vs-all encoding. In doing so, the images themselves remain unchanged, only the labels are transformed from multiclass to binary.\\

For the ten transformed training sets $S^{(a)}$, the goal is to train a binary classifier $h_{S^{(a)}}: U \rightarrow \{-1, 1\}, \vec{x} \mapsto \hat{z}$. Here, $\hat{z}$ is the predicted binary label for an arbitrary image $\vec{x}$. A well-known procedure to train $h_{S^{(a)}}$ using $S^{(a)}$ is the kernel perceptron algorithm that was first presented by Aizerman et al in $1964$ \cite{kernel1964}. In the present work, the algorithm is used in the following form:

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{$n_{epoch}$, $n_{sample}$, $p$, $S^{(a)}$}
 let $A = \emptyset$, $\vec{\alpha} = \vec{0}$\;
 \For{all $1, \dots, n_{epoch}$}{
 	let $T^{(a)} = \emptyset$\;
 	draw $n_{draw}$ examples from $S^{(a)}$ with replacement and store 		         	them in $T^{(a)}$ \;
	\For{all $t = 1, \dots, n_{sample}$}{
		compute $\hat{z} = \mathrm{sgn}\left(\sum_{s: \alpha_s \neq 0} \alpha_s z_s 	     	K_p(\vec{x_s}, \vec{x_t})\right)$\;
	\If{$\hat{z} \neq z_t$}{
		$\alpha_i \leftarrow \alpha_i + 1$ where $i$ is the index of example 		$(\vec{x_t}, z_t)$ in $S^{(a)}$\;
	}
	store $\vec{\alpha}$ in $A$\;
	}
  }
\Output{$A$}
\caption{Binary Kernel Perceptron}
\end{algorithm}

The algorithm has three inputs. $n_{epochs}$ describes the number of epochs for which the binary predictor should be trained. Each epoch considers a subset $T^{(a)} \subset S^{(a)}$ of the training examples that is generated by drawing examples at random with replacement from the complete training set $S^{(a)}$. This - instead of using the complete training set in one epoch - is basically done to insure computational feasability. In general, the binary predictor in the kernel perceptron algorithm at each step has the form
\begin{align*}
	h_{S^{(a)}} = \mathrm{sgn}(g_{S^{(a)}}),
\end{align*}
with
\begin{align*}
	g_{S^{(a)}} = \sum_{s: \alpha_s \neq 0} \alpha_s z_s 	     	K_p(\vec{x_s}, \vec{x_t}),
\end{align*}
and is completely described by a vector $\vec{\alpha}$. The binary kernel perceptron algorithm updates $\vec{\alpha}$, whenever its prediction $\hat{z}$ errs on a training example. Thus, $\vec{\alpha}$ counts how often each training example in $S^{(\alpha)}$ is missclassified.
The output of the algorithm is the set $A$ that contains all vectors $\vec{\alpha}$ that were produced by the binary kernel perceptron algorithm during the training phase.\\

After the application of the binary kernel perceptron algorithm, a predictor inside of all predictors $A$ has to be choosen. One could simply take the predictor at the end of the algorithm. However, there are more refined choices. Among those are the average predictor $\langle \vec{\alpha} \rangle$ over all predictors in $A$ as well as the predictor $\vec{\alpha}_{min}$ that minimizes the training error 
\begin{align*}
	\hat{\ell}(\vec{\alpha}) = \frac{1}{n_{epoch} n_{sample}} \sum_{(\vec{x}, z) \in S^{(a)}} \ell(h_{S^{(a)}}(\vec{x}), z),
\end{align*}
where $\ell$ is the zero-one loss 
\begin{align*}
	\ell(\hat{z}, z) = \mathbbm{1}(\hat{z} = z),
\end{align*}
where $\mathbbm{1}$ is the indicator function.
The binary kernel perceptron algorithm can be applied for all ten digits.

define kernel -> connection to classical perceptron algorithm
expansion to multiclass prediction
online algorithm