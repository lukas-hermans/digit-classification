\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{biblatex}
\bibdata{main-blx,bibliography}
\citation{biblatex-control}
\abx@aux@refcontext{none/global//global/global}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{USenglish}{}
\@writefile{toc}{\contentsline {section}{Declaration}{2}{section*.1}\protected@file@percent }
\citation{autonomeous2020}
\abx@aux@cite{autonomeous2020}
\abx@aux@segm{0}{0}{autonomeous2020}
\citation{emotion2020}
\abx@aux@cite{emotion2020}
\abx@aux@segm{0}{0}{emotion2020}
\citation{medicine2021}
\abx@aux@cite{medicine2021}
\abx@aux@segm{0}{0}{medicine2021}
\citation{MNIST}
\abx@aux@cite{MNIST}
\abx@aux@segm{0}{0}{MNIST}
\citation{KaggleData}
\abx@aux@cite{KaggleData}
\abx@aux@segm{0}{0}{KaggleData}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Dataset \& Definitions}{3}{section.2}\protected@file@percent }
\newlabel{sec:dataset_definitions}{{2}{3}{Dataset \& Definitions}{section.2}{}}
\newlabel{eq:vector}{{1}{3}{Dataset \& Definitions}{equation.2.1}{}}
\citation{multiclass2005}
\abx@aux@cite{multiclass2005}
\abx@aux@segm{0}{0}{multiclass2005}
\citation{kernel1964}
\abx@aux@cite{kernel1964}
\abx@aux@segm{0}{0}{kernel1964}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Examplary ensemble of ten different handwritten digits from $0$ to $9$ in the MNIST dataset. Each image can be described by a feature vector $\vec  {x}$, and has a human-decided label $y$. The images consist of $28 \times 28 = 784$ pixels, where each pixel encodes an integer brightness level between $0$ and $255$.\relax }}{4}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:plot_examples}{{1}{4}{Examplary ensemble of ten different handwritten digits from $0$ to $9$ in the MNIST dataset. Each image can be described by a feature vector $\vec {x}$, and has a human-decided label $y$. The images consist of $28 \times 28 = 784$ pixels, where each pixel encodes an integer brightness level between $0$ and $255$.\relax }{figure.caption.3}{}}
\newlabel{fig:plot_hist_train}{{2a}{4}{training examples\relax }{figure.caption.4}{}}
\newlabel{sub@fig:plot_hist_train}{{a}{4}{training examples\relax }{figure.caption.4}{}}
\newlabel{fig:plot_hist_test}{{2b}{4}{test examples\relax }{figure.caption.4}{}}
\newlabel{sub@fig:plot_hist_test}{{b}{4}{test examples\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The histograms show, how often each digit occurs in (a) the training set $S$ and (b) the test set $D$. Both sets are balanced as all of the ten digits appear with a similar frequency.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:plot_hist}{{2}{4}{The histograms show, how often each digit occurs in (a) the training set $S$ and (b) the test set $D$. Both sets are balanced as all of the ten digits appear with a similar frequency.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory}{4}{section.3}\protected@file@percent }
\newlabel{sec:theory}{{3}{4}{Theory}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}From Multiclass to Binary: One-vs-All Encoding}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Binary Kernel Perceptron Algorithm}{4}{subsection.3.2}\protected@file@percent }
\citation{perceptron1957}
\abx@aux@cite{perceptron1957}
\abx@aux@segm{0}{0}{perceptron1957}
\newlabel{eq:bin_predictor}{{2}{5}{Binary Kernel Perceptron Algorithm}{equation.3.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Binary Kernel Perceptron\relax }}{5}{algocf.1}\protected@file@percent }
\newlabel{alg:binary_kernel_perceptron}{{1}{5}{Binary Kernel Perceptron Algorithm}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Choice of Binary Predictors}{5}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:choice}{{3.3}{5}{Choice of Binary Predictors}{subsection.3.3}{}}
\citation{multiclass2005}
\abx@aux@cite{multiclass2005}
\abx@aux@segm{0}{0}{multiclass2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Multiclass Kernel Perceptron Algorithm}{6}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Evaluation: Training and Test Error Rate}{6}{subsection.3.5}\protected@file@percent }
\newlabel{eq:training_error}{{3}{6}{Evaluation: Training and Test Error Rate}{equation.3.3}{}}
\newlabel{eq:test_error}{{4}{6}{Evaluation: Training and Test Error Rate}{equation.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation \& Software}{7}{section.4}\protected@file@percent }
\newlabel{sec:implementation_software}{{4}{7}{Implementation \& Software}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{7}{section.5}\protected@file@percent }
\newlabel{sec:results}{{5}{7}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Binary Predictors}{7}{subsection.5.1}\protected@file@percent }
\newlabel{subsec:bin_pred}{{5.1}{7}{Binary Predictors}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Binary training error rates $\hat  {\ell }_{S^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary training error rates $\hat  {\ell }_{S^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary test error $\hat  {\ell }_{D^{(5)}}$ rates for the same binary predictors are displayed in Fig~\ref  {fig:test_error_bin}.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:train_error_bin}{{3}{8}{Binary training error rates $\hat {\ell }_{S^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary training error rates $\hat {\ell }_{S^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary test error $\hat {\ell }_{D^{(5)}}$ rates for the same binary predictors are displayed in Fig~\ref {fig:test_error_bin}.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Binary test error rates $\hat  {\ell }_{D^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary test error rates $\hat  {\ell }_{D^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary training error rates $\hat  {\ell }_{S^{(5)}}$ for the same binary predictors are displayed in Fig~\ref  {fig:train_error_bin}.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:test_error_bin}{{4}{9}{Binary test error rates $\hat {\ell }_{D^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary test error rates $\hat {\ell }_{D^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary training error rates $\hat {\ell }_{S^{(5)}}$ for the same binary predictors are displayed in Fig~\ref {fig:train_error_bin}.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multiclass Predictors}{10}{subsection.5.2}\protected@file@percent }
\newlabel{subsec:multi_pred}{{5.2}{10}{Multiclass Predictors}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Multiclass training error rates $\hat  {\ell }_{S}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass training error rates $\hat  {\ell }_{S}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass test error rates $\hat  {\ell }_{D}$ for the same multiclass predictors are displayed in Fig~\ref  {fig:test_error_multi}.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:train_error_multi}{{5}{11}{Multiclass training error rates $\hat {\ell }_{S}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass training error rates $\hat {\ell }_{S}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass test error rates $\hat {\ell }_{D}$ for the same multiclass predictors are displayed in Fig~\ref {fig:test_error_multi}.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Multiclass test error rates $\hat  {\ell }_{D}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass test error rates $\hat  {\ell }_{D}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass training error rates $\hat  {\ell }_{S}$ for the same multiclass predictors are displayed in Fig~\ref  {fig:train_error_multi}. In each panel, the vertical lines indicate the number of epochs $N_{epoch}$ with the smallest test error rate for all three predictor types. The predictors obtaining overall the lowest test error rates are reported in Tab.~\ref  {tab:min_test}.\relax }}{12}{figure.caption.8}\protected@file@percent }
\newlabel{fig:test_error_multi}{{6}{12}{Multiclass test error rates $\hat {\ell }_{D}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass test error rates $\hat {\ell }_{D}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass training error rates $\hat {\ell }_{S}$ for the same multiclass predictors are displayed in Fig~\ref {fig:train_error_multi}. In each panel, the vertical lines indicate the number of epochs $N_{epoch}$ with the smallest test error rate for all three predictor types. The predictors obtaining overall the lowest test error rates are reported in Tab.~\ref {tab:min_test}.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Minimizing the Test Error Rate}{13}{subsection.5.3}\protected@file@percent }
\newlabel{subsec:best_pred}{{5.3}{13}{Minimizing the Test Error Rate}{subsection.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Multiclass predictors that obtain the lowest test error rates $\hat  {\ell }_D$ on the test set $D$ of the MNIST database of handwritten digits applying the multiclass kernel perceptron algorithm. In the table, the degree $p$ of the polynomial kernel as well as the number of epochs $N_{epoch}$ for which the test error rate $\hat  {\ell }_D$ is minimized are stated for each of the three predictor types $\vec  {\alpha }_{fin}$, $\vec  {\alpha }_{min}$, and $\langle \vec  {\alpha }\rangle $. In the present work, kernel degrees from $p=1$ to $p=6$ for up to $N_{epoch}=10$ training cycles over random permuations in the training set $S$ were considered. For completeness, the corresponding training error rate $\hat  {\ell }_S$ is reported.\relax }}{13}{table.caption.9}\protected@file@percent }
\newlabel{tab:min_test}{{1}{13}{Multiclass predictors that obtain the lowest test error rates $\hat {\ell }_D$ on the test set $D$ of the MNIST database of handwritten digits applying the multiclass kernel perceptron algorithm. In the table, the degree $p$ of the polynomial kernel as well as the number of epochs $N_{epoch}$ for which the test error rate $\hat {\ell }_D$ is minimized are stated for each of the three predictor types $\vec {\alpha }_{fin}$, $\vec {\alpha }_{min}$, and $\langle \vec {\alpha }\rangle $. In the present work, kernel degrees from $p=1$ to $p=6$ for up to $N_{epoch}=10$ training cycles over random permuations in the training set $S$ were considered. For completeness, the corresponding training error rate $\hat {\ell }_S$ is reported.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{13}{section.6}\protected@file@percent }
\newlabel{sec:discussion}{{6}{13}{Discussion}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Confusion matrix for the multiclass predictor that minimizes the test error rate $\hat  {\ell }_D$ on the test set $D$ of the MNIST database, for (a) the final predictor $\vec  {\alpha }_{fin}$, (b) the minimizing predictor $\vec  {\alpha }_{min}$, and (c) the average predictor $\langle \vec  {\alpha }\rangle $, as reported in Tab.~\ref  {tab:min_test}.\relax }}{14}{figure.caption.10}\protected@file@percent }
\newlabel{fig:conf_mat}{{7}{14}{Confusion matrix for the multiclass predictor that minimizes the test error rate $\hat {\ell }_D$ on the test set $D$ of the MNIST database, for (a) the final predictor $\vec {\alpha }_{fin}$, (b) the minimizing predictor $\vec {\alpha }_{min}$, and (c) the average predictor $\langle \vec {\alpha }\rangle $, as reported in Tab.~\ref {tab:min_test}.\relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of the test error rate on the MNIST test set $D$ obtained by training the multiclass kernel perceptron algorithm with other image recognition techniques in the literature.\relax }}{14}{table.caption.11}\protected@file@percent }
\newlabel{tab:comparison}{{2}{14}{Comparison of the test error rate on the MNIST test set $D$ obtained by training the multiclass kernel perceptron algorithm with other image recognition techniques in the literature.\relax }{table.caption.11}{}}
\citation{dluncertainties2020}
\abx@aux@cite{dluncertainties2020}
\abx@aux@segm{0}{0}{dluncertainties2020}
\citation{mluncertainties2007}
\abx@aux@cite{mluncertainties2007}
\abx@aux@segm{0}{0}{mluncertainties2007}
\citation{bayesian2020}
\abx@aux@cite{bayesian2020}
\abx@aux@segm{0}{0}{bayesian2020}
\citation{EMNIST2017}
\abx@aux@cite{EMNIST2017}
\abx@aux@segm{0}{0}{EMNIST2017}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion \& Outlook}{15}{section.7}\protected@file@percent }
\newlabel{sec:conclusion_outlook}{{7}{15}{Conclusion \& Outlook}{section.7}{}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{autonomeous2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{emotion2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{medicine2021}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MNIST}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{KaggleData}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{multiclass2005}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kernel1964}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{perceptron1957}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{dluncertainties2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mluncertainties2007}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesian2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{EMNIST2017}{none/global//global/global}
\abx@aux@defaultlabelprefix{0}{autonomeous2020}{}
\abx@aux@defaultlabelprefix{0}{emotion2020}{}
\abx@aux@defaultlabelprefix{0}{medicine2021}{}
\abx@aux@defaultlabelprefix{0}{MNIST}{}
\abx@aux@defaultlabelprefix{0}{KaggleData}{}
\abx@aux@defaultlabelprefix{0}{multiclass2005}{}
\abx@aux@defaultlabelprefix{0}{kernel1964}{}
\abx@aux@defaultlabelprefix{0}{perceptron1957}{}
\abx@aux@defaultlabelprefix{0}{dluncertainties2020}{}
\abx@aux@defaultlabelprefix{0}{mluncertainties2007}{}
\abx@aux@defaultlabelprefix{0}{bayesian2020}{}
\abx@aux@defaultlabelprefix{0}{EMNIST2017}{}
\@writefile{toc}{\contentsline {section}{References}{16}{section.7}\protected@file@percent }
