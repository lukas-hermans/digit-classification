\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{biblatex}
\bibdata{main-blx,bibliography}
\citation{biblatex-control}
\abx@aux@refcontext{none/global//global/global}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{USenglish}{}
\@writefile{toc}{\contentsline {section}{Declaration}{2}{section*.1}\protected@file@percent }
\citation{autonomeous2020}
\abx@aux@cite{autonomeous2020}
\abx@aux@segm{0}{0}{autonomeous2020}
\citation{emotion2020}
\abx@aux@cite{emotion2020}
\abx@aux@segm{0}{0}{emotion2020}
\citation{medicine2021}
\abx@aux@cite{medicine2021}
\abx@aux@segm{0}{0}{medicine2021}
\citation{MNIST}
\abx@aux@cite{MNIST}
\abx@aux@segm{0}{0}{MNIST}
\citation{KaggleData}
\abx@aux@cite{KaggleData}
\abx@aux@segm{0}{0}{KaggleData}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Dataset \& Definitions}{3}{section.2}\protected@file@percent }
\newlabel{sec:dataset_definitions}{{2}{3}{Dataset \& Definitions}{section.2}{}}
\newlabel{eq:vector}{{1}{3}{Dataset \& Definitions}{equation.2.1}{}}
\citation{multiclass2005}
\abx@aux@cite{multiclass2005}
\abx@aux@segm{0}{0}{multiclass2005}
\citation{kernel1964}
\abx@aux@cite{kernel1964}
\abx@aux@segm{0}{0}{kernel1964}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Examplary ensemble of ten different handwritten digits from $0$ to $9$ in the MNIST dataset. Each image can be described by a feature vector $\vec  {x}$, and has a human-decided label $y$. The images consist of $28 \times 28 = 784$ pixels, where each pixel encodes an integer brightness level between $0$ and $255$.\relax }}{4}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:plot_examples}{{1}{4}{Examplary ensemble of ten different handwritten digits from $0$ to $9$ in the MNIST dataset. Each image can be described by a feature vector $\vec {x}$, and has a human-decided label $y$. The images consist of $28 \times 28 = 784$ pixels, where each pixel encodes an integer brightness level between $0$ and $255$.\relax }{figure.caption.3}{}}
\newlabel{fig:plot_hist_train}{{2a}{4}{training examples\relax }{figure.caption.4}{}}
\newlabel{sub@fig:plot_hist_train}{{a}{4}{training examples\relax }{figure.caption.4}{}}
\newlabel{fig:plot_hist_test}{{2b}{4}{test examples\relax }{figure.caption.4}{}}
\newlabel{sub@fig:plot_hist_test}{{b}{4}{test examples\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The histograms show, how often each digit occurs in (a) the training set $S$ and (b) the test set $D$. In both sets, all of the digits appear with a similar frequency, such that the training and test sets are balanced.\relax }}{4}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory}{4}{section.3}\protected@file@percent }
\newlabel{sec:theory}{{3}{4}{Theory}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}From Multiclass to Binary: One-vs-All Encoding}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Binary Kernel Perceptron Algorithm}{4}{subsection.3.2}\protected@file@percent }
\citation{perceptron1957}
\abx@aux@cite{perceptron1957}
\abx@aux@segm{0}{0}{perceptron1957}
\newlabel{eq:bin_predictor}{{2}{5}{Binary Kernel Perceptron Algorithm}{equation.3.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Binary Kernel Perceptron\relax }}{5}{algocf.1}\protected@file@percent }
\newlabel{alg:binary_kernel_perceptron}{{1}{5}{Binary Kernel Perceptron Algorithm}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Choice of Binary Predictors}{5}{subsection.3.3}\protected@file@percent }
\newlabel{subsec:choice}{{3.3}{5}{Choice of Binary Predictors}{subsection.3.3}{}}
\citation{multiclass2005}
\abx@aux@cite{multiclass2005}
\abx@aux@segm{0}{0}{multiclass2005}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Multiclass Kernel Perceptron Algorithm}{6}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Evaluation: Training and Test Error Rate}{6}{subsection.3.5}\protected@file@percent }
\newlabel{eq:training_error}{{3}{6}{Evaluation: Training and Test Error Rate}{equation.3.3}{}}
\newlabel{eq:test_error}{{4}{6}{Evaluation: Training and Test Error Rate}{equation.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation \& Software}{7}{section.4}\protected@file@percent }
\newlabel{sec:implementation_software}{{4}{7}{Implementation \& Software}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{7}{section.5}\protected@file@percent }
\newlabel{sec:results}{{5}{7}{Results}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Binary Predictors}{7}{subsection.5.1}\protected@file@percent }
\newlabel{subsec:bin_pred}{{5.1}{7}{Binary Predictors}{subsection.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Binary training error rates $\hat  {\ell }_{S^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary training error rates $\hat  {\ell }_{S^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary test error rates for the same binary predictors are displayed in Fig~\ref  {fig:test_error_bin}.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:train_error_bin}{{3}{8}{Binary training error rates $\hat {\ell }_{S^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary training error rates $\hat {\ell }_{S^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary test error rates for the same binary predictors are displayed in Fig~\ref {fig:test_error_bin}.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Binary test error rates $\hat  {\ell }_{D^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary test error rates $\hat  {\ell }_{D^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary training error rates for the same binary predictors are displayed in Fig~\ref  {fig:train_error_bin}.\relax }}{9}{figure.caption.6}\protected@file@percent }
\newlabel{fig:test_error_bin}{{4}{9}{Binary test error rates $\hat {\ell }_{D^{(5)}}$ as a function of the number of epochs $N_{epoch}$ for the training of binary predictors that classify, whether an image shows the digit $a=5$ or not. Each panel displays the binary test error rates $\hat {\ell }_{D^{(5)}}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors from the binary kernel perceptron algorithm. Note, the different scales of the vertical axes. The binary training error rates for the same binary predictors are displayed in Fig~\ref {fig:train_error_bin}.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multiclass Predictors}{10}{subsection.5.2}\protected@file@percent }
\newlabel{subsec:multi_pred}{{5.2}{10}{Multiclass Predictors}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Minimizing the Test Error Rate}{10}{subsection.5.3}\protected@file@percent }
\newlabel{subsec:best_pred}{{5.3}{10}{Minimizing the Test Error Rate}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{10}{section.6}\protected@file@percent }
\newlabel{sec:discussion}{{6}{10}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion \& Outlook}{10}{section.7}\protected@file@percent }
\newlabel{sec:conclusion_outlook}{{7}{10}{Conclusion \& Outlook}{section.7}{}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{autonomeous2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{emotion2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{medicine2021}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MNIST}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{KaggleData}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{multiclass2005}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kernel1964}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{perceptron1957}{none/global//global/global}
\abx@aux@defaultlabelprefix{0}{autonomeous2020}{}
\abx@aux@defaultlabelprefix{0}{emotion2020}{}
\abx@aux@defaultlabelprefix{0}{medicine2021}{}
\abx@aux@defaultlabelprefix{0}{MNIST}{}
\abx@aux@defaultlabelprefix{0}{KaggleData}{}
\abx@aux@defaultlabelprefix{0}{multiclass2005}{}
\abx@aux@defaultlabelprefix{0}{kernel1964}{}
\abx@aux@defaultlabelprefix{0}{perceptron1957}{}
\@writefile{toc}{\contentsline {section}{References}{11}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Multiclass training error rates $\hat  {\ell }_{S}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass training error rates $\hat  {\ell }_{S}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass test error rates for the same multiclass predictors are displayed in Fig~\ref  {fig:test_error_multi}.\relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:train_error_multi}{{5}{12}{Multiclass training error rates $\hat {\ell }_{S}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass training error rates $\hat {\ell }_{S}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass test error rates for the same multiclass predictors are displayed in Fig~\ref {fig:test_error_multi}.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Multiclass test error rates $\hat  {\ell }_{D}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass test error rates $\hat  {\ell }_{D}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass training error rates for the same multiclass predictors are displayed in Fig~\ref  {fig:train_error_multi}. In each panel, the vertical lines indicate the number of epochs $N_{epoch}$ with the smallest test error rate for all three predictor types. The results are also reported in Tab.~\ref  {tab:min_test}.\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:test_error_multi}{{6}{13}{Multiclass test error rates $\hat {\ell }_{D}$ as a function of the number of epochs $N_{epoch}$ for the training of multiclass predictors on the MNIST training set $S$. The multiclass predictors classify given images of handwritten digits with a label between $0$ and $9$. Each panel displays the multiclass test error rates $\hat {\ell }_{D}$ for a different degree $p$ of the polynomial kernel for the three approaches to retrieve binary predictors inside the multiclass kernel perceptron algorithm. Note, the different scales of the vertical axes. The multiclass training error rates for the same multiclass predictors are displayed in Fig~\ref {fig:train_error_multi}. In each panel, the vertical lines indicate the number of epochs $N_{epoch}$ with the smallest test error rate for all three predictor types. The results are also reported in Tab.~\ref {tab:min_test}.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces confusion matrix\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:conf_mat}{{7}{14}{confusion matrix\relax }{figure.caption.9}{}}
