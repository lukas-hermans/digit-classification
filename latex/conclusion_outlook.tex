In the present work, three realizations of the multiclass kernel perceptron algorithm lead to test error rates of \SI{1.54}{\percent}, \SI{1.55}{\percent}, and \SI{1.58}{\percent} on the test set of the MNIST database. This result compares well with other classical machine learning approaches such as support vector machines or K-nearest-neighbor algortihms. In general, complex neural networks have a better performance. The obtained test error rate of the multiclass kernel perceptron algorithm might further decrease, when preprocessing (e.g. rotating, blurring) the images prior to the training of the multiclass predictors. 

It has been seen that the multiclass kernel perceptron algorithm has the major advantage that it is an online learning algorithm that handles growing databases well. The main drawback is a missing quantification of the uncertainties on the predictions that might be critical in some areas of application.\\

Further research regards the expansion of the predictors beyond the case of the MNIST database, e.g. to include letters from $A$ to $Z$ \cite{EMNIST2017} It might also be of interest to include a class \enquote{invalid input} for images that can not be classified as a certain digit. This could reduce the number of missclassified digits in some applications.